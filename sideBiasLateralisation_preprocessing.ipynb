{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takes <30s. However, it might take >1 mins or so if network is busy...\n",
      "Matlab engine is set correctly.\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING CODE FOR IMAGING DATA with Behaviour\n",
    "# Creates CSV, aligned the time events, runs suite2p and calculate dff for all cells\n",
    "# Creates an analysis folder & saves necessary files for each recording session\n",
    "# # This code creates necessary behaviour & imaging files\n",
    "# It should work for any given task recorded in Dual2p or Packer 1 scope\n",
    "# Requires MATLAB integration, and uses getBehavData matlab function\n",
    "# \n",
    "# 30/08/2025 HA\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#make data file\n",
    "print( \"Takes <30s. However, it might take >1 mins or so if network is busy...\")\n",
    "import glob\n",
    "import sys,os, glob, shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import main_funcs as mfun\n",
    "import plot_funcs as pfun\n",
    "import utils_funcs as utils # utils is from Vape - catcher file: \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import pickle\n",
    "import time\n",
    "from LakLabAnalysis.Utility.extract_paq_events import extract_paq_data_frame\n",
    "import LakLabAnalysis.Utility.utils_funcs as utils_laklab\n",
    "from tifffile import TiffFile, imread, imwrite\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# set matlab API\n",
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "print('Matlab engine is set correctly.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  margin:14px 10px; padding:14px 18px;\n",
    "  background:linear-gradient(90deg,#7c3aed,#06b6d4);\n",
    "  color:white; font-size:24px; font-weight:800;\n",
    "  border-radius:10px; letter-spacing:.3px;\">\n",
    "  ⬇️ \n",
    "  Preprocessing for GOOD performance + Imaged session\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions required below\n",
    "def create_FOV_withSelectedCells(isCell, ops, s2p_path):\n",
    "    prob_threshold = 0\n",
    "    cell_indices = np.where((isCell[:,1] > prob_threshold))[0]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Show mean image\n",
    "    ax.imshow(ops['meanImg'], cmap='binary_r')\n",
    "\n",
    "    # Load stat.npy to get cell coordinates\n",
    "    stat = np.load(os.path.join(s2p_path, 'stat.npy'), allow_pickle=True)\n",
    "\n",
    "    # Generate random colors for each cell\n",
    "    colors = np.random.rand(len(cell_indices), 3)\n",
    "\n",
    "    # Draw ROIs of filtered cells\n",
    "    for idx, cell_number in enumerate(cell_indices):\n",
    "        stat_cell = stat[cell_number]\n",
    "        ypix = [stat_cell['ypix'][i] for i in range(len(stat_cell['ypix'])) if not stat_cell['overlap'][i]]\n",
    "        xpix = [stat_cell['xpix'][i] for i in range(len(stat_cell['xpix'])) if not stat_cell['overlap'][i]]\n",
    "        ax.plot(xpix, ypix, '.', markersize=1, alpha=0.7, color=colors[idx])\n",
    "\n",
    "    ax.set_title(f\"Detected cells (prob > {prob_threshold}): {len(cell_indices)}\")\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    fig.savefig(\n",
    "        os.path.join(info.recordingList.analysispathname[ind], f\"MeanImG.png\"),\n",
    "        dpi=300,          # higher resolution\n",
    "        bbox_inches='tight',  # trim whitespace\n",
    "        pad_inches=0.1,       # small padding\n",
    "        facecolor='white'     # ensure background matches\n",
    "    )\n",
    "\n",
    "    # Also show enhanced image (meanImgE) which might be clearer\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.imshow(ops['meanImgE'], cmap='binary_r')\n",
    "\n",
    "    # Draw ROIs of filtered cells\n",
    "    for idx, cell_number in enumerate(cell_indices):\n",
    "        stat_cell = stat[cell_number]\n",
    "        ypix = [stat_cell['ypix'][i] for i in range(len(stat_cell['ypix'])) if not stat_cell['overlap'][i]]\n",
    "        xpix = [stat_cell['xpix'][i] for i in range(len(stat_cell['xpix'])) if not stat_cell['overlap'][i]]\n",
    "        ax.plot(xpix, ypix, '.', markersize=1, alpha=0.7, color=colors[idx])\n",
    "\n",
    "    ax.set_title(f\"Detected cells (prob > {prob_threshold}): {len(cell_indices)}\")\n",
    "    ax.axis('off')\n",
    "    # save this image in the analysis folder\n",
    "    fig.savefig(\n",
    "        os.path.join(info.recordingList.analysispathname[ind], f\"FOV_withSelectedCells_{prob_threshold}.png\"),\n",
    "        dpi=300,          # higher resolution\n",
    "        bbox_inches='tight',  # trim whitespace\n",
    "        pad_inches=0.1,       # small padding\n",
    "        facecolor='white'     # ensure background matches\n",
    "    )\n",
    "    #close the plot\n",
    "    plt.close(fig)\n",
    "\n",
    "def calculateHeatmapsForRewardedStimulus():\n",
    "   ## Parameters\n",
    "   fRate = 1000/30\n",
    "   responsiveness_test_duration = 1000.0 #in ms \n",
    "   pre_frames    = 2000.0# in ms\n",
    "   pre_frames    = int(np.ceil(pre_frames/fRate))\n",
    "   post_frames   = 6000.0 # in ms\n",
    "   post_frames   = int(np.ceil(post_frames/fRate))\n",
    "   analysisWindowDur = 750 # in ms\n",
    "   analysisWindowDur = int(np.ceil(analysisWindowDur/fRate))\n",
    "   duration ='5'\n",
    "\n",
    "   #Create a huge dictionary with all cells and parameters for each cell\n",
    "   pathname = info.recordingList.analysispathname[ind]\n",
    "\n",
    "   ########## Organise stimuli times \n",
    "   filenameCSV = info.recordingList.analysispathname[ind] + info.recordingList.sessionName[ind] + '_CorrectedeventTimes.csv'\n",
    "   filenameCSV = [f for f in glob.glob(filenameCSV)]    \n",
    "   behData     = pd.read_csv(filenameCSV[0], header=0)\n",
    "   visTimes    = behData['stimulusOnsetTime'] + behData['trialOffsets']\n",
    "   rewardTimes = behData['rewardTime'] + behData['trialOffsets']\n",
    "   choiceTimes = behData['choiceStartTime'] + behData['trialOffsets']\n",
    "\n",
    "   # Make a variable for rewarded trials\n",
    "   rewarded    =  behData['rewardTime'].notna() # True for rewarded which also means correct response\n",
    "   # Calculate reward Time for non-rewarded Trials\n",
    "   choiceCompleteTimes = behData['choiceCompleteTime'] + behData['trialOffsets']\n",
    "   diff_time = np.nanmean(rewardTimes -choiceTimes)\n",
    "   nan_indices = np.isnan(rewardTimes) # unrewarded trials\n",
    "   rewardTimes[nan_indices] = choiceCompleteTimes[nan_indices] + diff_time\n",
    "\n",
    "   # Get the stim start times \n",
    "   filenameTXT = os.path.join(info.recordingList.path[ind],'twoP') +'\\*_imaging_frames.txt'\n",
    "   filenameTXT= [f for f in glob.glob(filenameTXT)]    \n",
    "   frame_clock = pd.read_csv(filenameTXT[0],  header= None)\n",
    "      \n",
    "   stimFrameTimes    = utils.stim_start_frame_Dual2Psetup (frame_clock, visTimes)\n",
    "   rewardFrameTimes  = utils.stim_start_frame_Dual2Psetup (frame_clock, rewardTimes)\n",
    "\n",
    "   ########## Organise calcium imaging traces \n",
    "   imData = pd.read_pickle (pathname +'imaging-data.pkl')\n",
    "   fluR      = imData['flu']\n",
    "   stat      = imData['stat']\n",
    "   \n",
    "   flu = utils_laklab.preprocess_flu(fluR, smooth_method='savgol', do_zscore=False, smooth_first=True)         \n",
    "   flu_zscored = zscore(flu, axis=1) # Z-score for each neuron (across time)\n",
    "   flu_normalised = zscore (fluR)\n",
    "   #flu_normalised = mfun.norm_to_zero_one (flu)# Explore dynamics in each sessions\n",
    "\n",
    "   dffTrace_reward ={} \n",
    "   dffTrace_mean_reward ={}\n",
    "   dffTrace_stimuli ={} \n",
    "   dffTrace_mean_stimuli ={}\n",
    "\n",
    "   ### Get dff values for 2 time windows\n",
    "   tTypesName = ['Rewarded','Unrewarded']\n",
    "   tTypes = [(rewarded==True),(rewarded!=True) ]\n",
    "\n",
    "   for indx, t in enumerate(tTypesName):\n",
    "      # For reward aligned\n",
    "      selected_indices = [rewardFrameTimes[i] for i in np.where((tTypes[indx]==True)& (~np.isnan(rewardFrameTimes)))[0]]  \n",
    "      selected_indices = [value for value in selected_indices if value == value]\n",
    "      dffTrace_reward[t] = utils.flu_splitter(flu,selected_indices, pre_frames, post_frames)  # Cell x time x trial\n",
    "      dffTrace_mean_reward[t] = np.mean(dffTrace_reward[t],2) if len(selected_indices)>2 else None # Cell x time\n",
    "            \n",
    "      # For stimuli aligned\n",
    "      selected_indices = [stimFrameTimes[i] for i in np.where(tTypes[indx]==True)[0]]\n",
    "      selected_indices = [value for value in selected_indices if value == value]\n",
    "      dffTrace_stimuli[t] = utils.flu_splitter(flu_zscored, selected_indices, pre_frames, post_frames) # Cell x time x trial \n",
    "      dffTrace_mean_stimuli[t] = np.mean(dffTrace_stimuli[t],2) if len(selected_indices)>2 else None # Cell x time\n",
    "               # Heat Plots\n",
    "   colormap = 'viridis'\n",
    "   selectedSession = 'WithinSession'\n",
    "   save_path = info.recordingList.analysispathname[ind]\n",
    "   analysis_params = ['Rewarded', 'Unrewarded']\n",
    "\n",
    "   savefigname = 'FirstCheck-heatmap-rewardAligned_flu_' + str(duration[0]) + 'sec'\n",
    "   pfun.heatmap_sessions(dffTrace_mean_reward, analysis_params, colormap,\n",
    "                        selectedSession, duration, savefigname, save_path) \n",
    "   plt.close()\n",
    "\n",
    "   savefigname = 'FirstCheck-heatmap-stimuliAligned_fluZscored_' + str(duration[0]) + 'sec'\n",
    "   pfun.heatmap_sessions(dffTrace_mean_stimuli, analysis_params, colormap,\n",
    "                        selectedSession, duration, savefigname, save_path) \n",
    "   plt.close()\n",
    "         \n",
    "   # Mean Plots \n",
    "   zscoreRun = False \n",
    "   baseline_subtract = [-1.0, 0.0]  # Baseline window: 1 second before stimulus onset\n",
    "\n",
    "   # Get animal number and session info\n",
    "   session_name = info.recordingList.sessionName[ind]\n",
    "   title_prefix = f'{session_name}'\n",
    "\n",
    "   colormap = ['red', 'black']\n",
    "   savefigname = 'FirstCheck-mean-rewardAligned_' + str(duration[0]) + 'sec'\n",
    "   pfun.lineplot_sessions(dffTrace_mean_reward, analysis_params, colormap,\n",
    "                        duration, zscoreRun, savefigname, save_path, baseline_subtract, \n",
    "                        title=f'{title_prefix} - Rewarded vs Unrewarded') \n",
    "   plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGE the animal ID  & selection criteria. Performance works or 2AFC contrast taks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: decMaking310\n",
      "Computer: Huriye Windows\n",
      "Most likely expRef does not match Z:\\MBL014\\2025-06-27\\1\\2025-06-27_1_MBL014_Block.mat: too many indices for array: array is 0-dimensional, but 1 were indexed\n",
      "Total Session fits the selection: 11\n"
     ]
    }
   ],
   "source": [
    "# Get the list to extract the files for further analysis\n",
    "animalList= ['MBL014']\n",
    "info = mfun.analysis(animalList=animalList)\n",
    "info.recordingList = info.recordingList[\n",
    "    (info.recordingList['performance'] > 60) &\n",
    "    (info.recordingList['twoP']) &\n",
    "    (info.recordingList['duration'] > 20)\n",
    "    ].reset_index(drop=True)\n",
    "print('Total Session fits the selection: ' +  str(info.recordingList.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### 0 #### Preprocessing started for : 2025-06-12_1_MBL014 \n",
      "\n",
      "Step 1: Processing paq and creating imaging_frames.txt\n",
      "    .txt file has found. Skipping...\n",
      "    paq-data.pkl file has found. Skipping...\n",
      "Step 2: Creating CSV file\n",
      "    CorrectedeventTimes.csv file has found. Skipping...\n",
      "Step 3: check suite2p\n",
      "    Reference avg already exists. Skipping...\n",
      "    suite2p not found. Running extraction for: Z:\\MBL014\\2025-06-12\\TwoP\\2025-06-12_t-001\n",
      "29.873733883942098\n",
      "Z:\\MBL014\\2025-06-12\\TwoP\\2025-06-12_t-001\n",
      "CuPy version: 13.6.0\n",
      "CUDA device: b'NVIDIA GeForce RTX 3080'\n",
      "Ops useGPU: True\n",
      "{'data_path': 'Z:\\\\MBL014\\\\2025-06-12\\\\TwoP\\\\2025-06-12_t-001', 'tiff_list': ['Z:\\\\MBL014\\\\2025-06-12\\\\TwoP\\\\2025-06-12_t-001\\\\2025-06-12_t-001_Cycle00001_Ch2.tif'], 'save_path': 'Z:\\\\MBL014\\\\2025-06-12\\\\TwoP\\\\2025-06-12_t-001'}\n",
      "FOUND BINARIES AND OPS IN ['Z:\\\\MBL014\\\\2025-06-12\\\\TwoP\\\\2025-06-12_t-001\\\\suite2p\\\\plane0\\\\ops.npy']\n",
      "removing previous detection and extraction files, if present\n",
      ">>>>>>>>>>>>>>>>>>>>> PLANE 0 <<<<<<<<<<<<<<<<<<<<<<\n",
      "NOTE: not running registration, plane already registered\n",
      "binary path: Z:\\MBL014\\2025-06-12\\TwoP\\2025-06-12_t-001\\suite2p\\plane0\\data.bin\n",
      "NOTE: applying default C:\\Users\\Huriye\\.suite2p\\classifiers\\classifier_user.npy\n",
      "----------- ROI DETECTION\n",
      "Binning movie in chunks of length 38\n",
      "Binned movie of size [2705,442,468] created in 562.15 sec.\n",
      ">>>> CELLPOSE finding masks in enhanced_mean_img\n",
      "!NOTE! diameter set to 5.00 for cell detection with cellpose\n",
      "pretrained model C:\\Users\\Huriye\\.cellpose\\models\\cpsam not found, using default model\n",
      "Resizing is depricated in v4.0.1+\n",
      ">>>> 301 masks detected, median diameter = 14.93 \n",
      "Detected 301 ROIs, 8.31 sec\n",
      "After removing overlaps, 301 ROIs remain\n",
      "----------- Total 572.96 sec.\n",
      "----------- EXTRACTION\n",
      "Masks created, 1.39 sec.\n",
      "Extracted fluorescence from 301 ROIs in 102809 frames, 96.78 sec.\n",
      "----------- Total 100.40 sec.\n",
      "----------- CLASSIFICATION\n",
      "['npix_norm', 'skew', 'compact']\n",
      "----------- SPIKE DECONVOLUTION\n",
      "----------- Total 1.41 sec.\n",
      "moving binary files to save_path\n",
      "Plane 0 processed in 679.55 sec (can open in GUI).\n",
      "total = 693.28 sec.\n",
      "TOTAL RUNTIME 693.28 sec\n",
      "✓ Done in 693.3s → Z:/MBL014/2025-06-12/TwoP/2025-06-12_t-001/plane0\n",
      "Step 4: calculate & create imaging.pkl\n",
      "    Loading suite2p data...\n",
      "loading 54 traces labelled as cells\n",
      "subtracting neuropil with a coefficient of 0.7\n",
      "54 cells; 102809 frames\n",
      "    Successfully saved imaging data.\n",
      "C:\\Users\\Huriye\\AppData\\Local\\Temp\\ipykernel_20944\\2982280922.py:27: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "    Successfully saved FOV with selected cells.\n",
      "    Successfully saved heatmaps.\n",
      "0 Successfully completed in 753.6 seconds\n",
      "\n",
      "\n",
      "#### 1 #### Preprocessing started for : 2025-06-13_3_MBL014 \n",
      "\n",
      "Step 1: Processing paq and creating imaging_frames.txt\n",
      "    .txt file has found. Skipping...\n",
      "    paq-data.pkl file has found. Skipping...\n",
      "Step 2: Creating CSV file\n",
      "    CorrectedeventTimes.csv file has found. Skipping...\n",
      "Step 3: check suite2p\n",
      "    Reference avg already exists. Skipping...\n",
      "    suite2p not found. Running extraction for: Z:\\MBL014\\2025-06-13\\TwoP\\2025-06-13_t-001\n",
      "15.110550677575981\n",
      "Z:\\MBL014\\2025-06-13\\TwoP\\2025-06-13_t-001\n",
      "CuPy version: 13.6.0\n",
      "CUDA device: b'NVIDIA GeForce RTX 3080'\n",
      "Ops useGPU: True\n",
      "{'data_path': 'Z:\\\\MBL014\\\\2025-06-13\\\\TwoP\\\\2025-06-13_t-001', 'tiff_list': ['Z:\\\\MBL014\\\\2025-06-13\\\\TwoP\\\\2025-06-13_t-001\\\\2025-06-13_t-001_Cycle00001_Ch2.tif'], 'save_path': 'Z:\\\\MBL014\\\\2025-06-13\\\\TwoP\\\\2025-06-13_t-001'}\n",
      "FOUND BINARIES AND OPS IN ['Z:\\\\MBL014\\\\2025-06-13\\\\TwoP\\\\2025-06-13_t-001\\\\suite2p\\\\plane0\\\\ops.npy']\n",
      "removing previous detection and extraction files, if present\n",
      ">>>>>>>>>>>>>>>>>>>>> PLANE 0 <<<<<<<<<<<<<<<<<<<<<<\n",
      "NOTE: not running registration, plane already registered\n",
      "binary path: Z:\\MBL014\\2025-06-13\\TwoP\\2025-06-13_t-001\\suite2p\\plane0\\data.bin\n",
      "NOTE: applying default C:\\Users\\Huriye\\.suite2p\\classifiers\\classifier_user.npy\n",
      "----------- ROI DETECTION\n",
      "Binning movie in chunks of length 19\n",
      "Binned movie of size [2848,416,416] created in 286.46 sec.\n",
      ">>>> CELLPOSE finding masks in enhanced_mean_img\n",
      "!NOTE! diameter set to 5.00 for cell detection with cellpose\n",
      "pretrained model C:\\Users\\Huriye\\.cellpose\\models\\cpsam not found, using default model\n",
      "Resizing is depricated in v4.0.1+\n",
      ">>>> 115 masks detected, median diameter = 16.39 \n",
      "Detected 115 ROIs, 6.91 sec\n",
      "After removing overlaps, 115 ROIs remain\n",
      "----------- Total 294.83 sec.\n",
      "----------- EXTRACTION\n",
      "Masks created, 1.11 sec.\n",
      "Extracted fluorescence from 115 ROIs in 54113 frames, 45.25 sec.\n",
      "----------- Total 47.60 sec.\n",
      "----------- CLASSIFICATION\n",
      "['npix_norm', 'skew', 'compact']\n",
      "----------- SPIKE DECONVOLUTION\n",
      "----------- Total 0.28 sec.\n",
      "moving binary files to save_path\n",
      "Plane 0 processed in 343.87 sec (can open in GUI).\n",
      "total = 350.58 sec.\n",
      "TOTAL RUNTIME 350.58 sec\n",
      "✓ Done in 350.6s → Z:/MBL014/2025-06-13/TwoP/2025-06-13_t-001/plane0\n",
      "Step 4: calculate & create imaging.pkl\n",
      "    Loading suite2p data...\n",
      "loading 41 traces labelled as cells\n",
      "subtracting neuropil with a coefficient of 0.7\n",
      "41 cells; 54113 frames\n",
      "    Successfully saved imaging data.\n",
      "C:\\Users\\Huriye\\AppData\\Local\\Temp\\ipykernel_20944\\2982280922.py:27: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "    Successfully saved FOV with selected cells.\n",
      "    Successfully saved heatmaps.\n",
      "1 Successfully completed in 386.7 seconds\n",
      "\n",
      "\n",
      "#### 2 #### Preprocessing started for : 2025-06-18_1_MBL014 \n",
      "\n",
      "Step 1: Processing paq and creating imaging_frames.txt\n",
      "    .txt file has found. Skipping...\n",
      "    paq-data.pkl file has found. Skipping...\n",
      "Step 2: Creating CSV file\n",
      "    CorrectedeventTimes.csv file has found. Skipping...\n",
      "Step 3: check suite2p\n",
      "    Reference avg already exists. Skipping...\n",
      "    suite2p not found. Running extraction for: Z:\\MBL014\\2025-06-18\\TwoP\\2025-06-18_t-001\n",
      "29.873733883942098\n",
      "Z:\\MBL014\\2025-06-18\\TwoP\\2025-06-18_t-001\n",
      "CuPy version: 13.6.0\n",
      "CUDA device: b'NVIDIA GeForce RTX 3080'\n",
      "Ops useGPU: True\n",
      "{'data_path': 'Z:\\\\MBL014\\\\2025-06-18\\\\TwoP\\\\2025-06-18_t-001', 'tiff_list': ['Z:\\\\MBL014\\\\2025-06-18\\\\TwoP\\\\2025-06-18_t-001\\\\2025-06-18_t-001_Cycle00001_Ch2.tif'], 'save_path': 'Z:\\\\MBL014\\\\2025-06-18\\\\TwoP\\\\2025-06-18_t-001'}\n",
      "FOUND BINARIES AND OPS IN ['Z:\\\\MBL014\\\\2025-06-18\\\\TwoP\\\\2025-06-18_t-001\\\\suite2p\\\\plane0\\\\ops.npy']\n",
      "removing previous detection and extraction files, if present\n",
      ">>>>>>>>>>>>>>>>>>>>> PLANE 0 <<<<<<<<<<<<<<<<<<<<<<\n",
      "NOTE: not running registration, plane already registered\n",
      "binary path: Z:\\MBL014\\2025-06-18\\TwoP\\2025-06-18_t-001\\suite2p\\plane0\\data.bin\n",
      "NOTE: applying default C:\\Users\\Huriye\\.suite2p\\classifiers\\classifier_user.npy\n",
      "----------- ROI DETECTION\n",
      "Binning movie in chunks of length 38\n",
      "Binned movie of size [3007,416,416] created in 606.04 sec.\n",
      ">>>> CELLPOSE finding masks in enhanced_mean_img\n",
      "!NOTE! diameter set to 5.00 for cell detection with cellpose\n",
      "pretrained model C:\\Users\\Huriye\\.cellpose\\models\\cpsam not found, using default model\n",
      "Resizing is depricated in v4.0.1+\n",
      ">>>> 119 masks detected, median diameter = 15.39 \n",
      "Detected 119 ROIs, 7.37 sec\n",
      "After removing overlaps, 119 ROIs remain\n",
      "----------- Total 615.42 sec.\n",
      "----------- EXTRACTION\n",
      "Masks created, 0.93 sec.\n"
     ]
    }
   ],
   "source": [
    "# Extract information from raw data files - Creates frames.txt, CSV, suite2p & imaging-data.pkl \n",
    "csv_reCalculate = True # To calculate the missing CSV\n",
    "suite2p_reCalculate = True # To calculate the missing suite2p folder\n",
    "dff_reCalculate = True\n",
    "\n",
    "logfile = os.path.join(info.rootPath, 'Preprocessing_log_' +  animalList[0] + '.txt')\n",
    "tee = mfun.Tee(logfile)\n",
    "try:\n",
    "    for ind, recordingDate in enumerate(info.recordingList.recordingDate):\n",
    "        if ind >=0:\n",
    "            start_time = time.time()\n",
    "            print(f\"\\n#### {ind} #### Preprocessing started for : {info.recordingList.sessionName[ind]} \",  end=\"\\n\\n\")\n",
    "\n",
    "            ###### STEP 1: PAQ ///// Extract time events from PAQ file   Z:\\MBL014\\2025-06-13\\TwoP\\2025-06-13_MBL014_3_paq_imaging_frames.txt\n",
    "            print(f\"Step 1: Processing paq and creating imaging_frames.txt\")\n",
    "            filenamePAQextracted = glob.glob(os.path.join(info.recordingList.path[ind], \"**\", \"*paq_imaging_frames.txt\"))\n",
    "            if len(filenamePAQextracted)>0:\n",
    "                print('    .txt file has found. Skipping...')\n",
    "                info.recordingList.loc[ind,'PAQextracted']=1\n",
    "            else:\n",
    "                paq_path  = os.path.join(info.recordingList.path[ind], \"**\", \"*.paq\")\n",
    "                paq_files = glob.glob(paq_path, recursive=True)\n",
    "                if len(paq_files) == 1:\n",
    "                    try:\n",
    "                        extract_paq_data_frame(paq_files[0])\n",
    "                        info.recordingList.loc[ind,'PAQextracted']=1\n",
    "                    except KeyError as e:\n",
    "                        info.recordingList.loc[ind,'PAQextracted']=0\n",
    "                        print(f\"Error processing {paq_files[0]}: {e}\")\n",
    "                elif len(paq_files) > 1:\n",
    "                    print(f\"    Multiple PAQ files found for : {info.recordingList.sessionName[ind]}\")\n",
    "                    print(filenamePAQextracted)\n",
    "                    info.recordingList.loc[ind,'PAQextracted']=0\n",
    "                    continue \n",
    "                else:\n",
    "                    print(f\"    No PAQ files found for : {info.recordingList.sessionName[ind]}\")\n",
    "                    info.recordingList.loc[ind,'PAQextracted']=0\n",
    "                    continue \n",
    "\n",
    "            ##### check in analysis folder for paq-data.pkl\n",
    "            filenamePAQdata = glob.glob(os.path.join(info.recordingList.analysispathname[ind], \"paq-data.pkl\"))\n",
    "            if len(filenamePAQdata) > 0:\n",
    "                print('    paq-data.pkl file has found. Skipping...')\n",
    "                info.recordingList.loc[ind,'PAQdataFound']=1\n",
    "            else:\n",
    "                try: \n",
    "                    if not os.path.exists(info.recordingList.analysispathname.iloc[ind]) :\n",
    "                        os.makedirs(info.recordingList.analysispathname.iloc[ind])\n",
    "\n",
    "                    print('    calculating paq-data.pkl file ...')\n",
    "                    paq_path  = os.path.join(info.recordingList.path[ind], \"**\", \"*.paq\")\n",
    "                    paq_files = glob.glob(paq_path, recursive=True)\n",
    "                    paq_data = utils.paq_read( file_path=paq_files[0], plot=True, save_path=info.recordingList.analysispathname[ind])\n",
    "                    filenamePAQ_analysis = os.path.join(info.recordingList.analysispathname[ind], 'paq-data.pkl')\n",
    "                    with open(filenamePAQ_analysis, 'wb') as f:\n",
    "                        pickle.dump(paq_data, f)\n",
    "                    del paq_data\n",
    "                    info.recordingList.loc[ind,'PAQextracted']=1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {paq_files[0]}: {e}\")\n",
    "                    info.recordingList.loc[ind,'PAQextracted']=0\n",
    "\n",
    "            ### >>> We have PAQ file properly & now can do analysis on this recording - Lets create a folder for analysis preprocessing files\n",
    "            if info.recordingList.loc[ind,'PAQextracted'] !=1:\n",
    "                info.recordingList.loc[ind,'CSVcreated']=0\n",
    "                continue \n",
    "        \n",
    "            ###### STEP 2: CSV ///// create CSV for behaviour outcome\n",
    "            print('Step 2: Creating CSV file')\n",
    "            filenameCSV = info.recordingList.analysispathname[ind] + info.recordingList.sessionName[ind] + '_CorrectedeventTimes.csv'\n",
    "            e_filenameCSV = [f for f in glob.glob(filenameCSV)]\n",
    "            if len(e_filenameCSV)==1:\n",
    "                print('    CorrectedeventTimes.csv file has found. Skipping...')\n",
    "                info.recordingList.loc[ind,'CSVcreated']=1\n",
    "                info.recordingList.loc[ind,'CSVpath'] = filenameCSV\n",
    "            else:\n",
    "                if  (not csv_reCalculate) & (info.recordingList.loc[ind,'PAQextracted'] == 0):\n",
    "                    info.recordingList.loc[ind,'CSVcreated']=0\n",
    "                    info.recordingList.loc[ind,'CSVpath'] = filenameCSV\n",
    "                else:\n",
    "                    try: \n",
    "                        info.recordingList.loc[ind,'CSVpath'] = filenameCSV\n",
    "                        filenameTimeline = [f for f in glob.glob(info.recordingList.filepathname[ind]+ '\\\\' + info.recordingList.sessionName[ind] + '_Timeline.mat')]\n",
    "                        if  (len(filenameTimeline)>0):\n",
    "                            sessionProfile ='Grating2AFC'\n",
    "                        else:\n",
    "                            sessionProfile ='Grating2AFC_noTimeline'\n",
    "                            \n",
    "                        # Get behaviour trial data from Block.mat file\n",
    "                        data = eng.getBehavData(info.recordingList.sessionName[ind],sessionProfile)\n",
    "\n",
    "                        # Apply correction based on the weights to match the eventTimes\n",
    "                        if (len(filenameTimeline)>0) :\n",
    "                            print('    Aligning time events:', end=\"\", flush=True)\n",
    "                            # Get weights to convert from probe to behavioural timebase\n",
    "                            twoPpath = info.recordingList.path[ind] + '\\\\TwoP'\n",
    "                            sessionName = info.recordingList.sessionName[ind]\n",
    "                            figsavepath = info.recordingList.analysispathname[ind]\n",
    "                            dataCorrected, variance = eng.applySubtractionCorrection (data, twoPpath ,sessionName, True, figsavepath, nargout=2)\n",
    "                            eng.close('all', nargout=0) \n",
    "                            info.recordingList.loc[ind,'variance'] = variance\n",
    "\n",
    "                            # Apply correction to the signal\n",
    "                            data = dataCorrected\n",
    "                                \n",
    "                        # Save the file\n",
    "                        eng.writetable(data, filenameCSV, nargout=0)\n",
    "                        print(\"Completed.\")\n",
    "                        info.recordingList.loc[ind,'CSVcreated']=1\n",
    "                    except Exception as e:\n",
    "                        print(str(ind) + ' !!!!! FAILED: Creating CSV: ' + info.recordingList.sessionName[ind])\n",
    "                        info.recordingList.loc[ind,'CSVcreated']=0\n",
    "\n",
    "            ### >>> Only if we have CSV file properly - then we are ready for imaging data analysis\n",
    "            if info.recordingList.loc[ind,'CSVcreated'] !=1:\n",
    "                info.recordingList.loc[ind, 'suite2Pcreated'] = 0\n",
    "                info.recordingList.loc[ind, 'dffcreated'] = 0\n",
    "                continue\n",
    "\n",
    "            ###### STEP 3: check recording quality - extract cells ROIS with suite2p \n",
    "            print('Step 3: check suite2p')\n",
    "            twoP_dir = os.path.dirname(info.recordingList.imagingTiffFileNames[ind])\n",
    "            tiff_path    = info.recordingList.imagingTiffFileNames[ind]\n",
    "            filenameTiff = glob.glob(os.path.join(tiff_path, \"*Ch2.tif\"))\n",
    "            if len(filenameTiff) == 0:\n",
    "                print(f\"TIFF missing/invalid for index {ind}: {tiff_path}\")\n",
    "                info.recordingList.loc[ind, 'suite2Pcreated'] = 0\n",
    "                continue\n",
    "\n",
    "            ref_dir = os.path.join(tiff_path, \"References\")\n",
    "            ref_matches = glob.glob(os.path.join(ref_dir, \"*-Ch2-16bit-Reference.tif\"))\n",
    "            \n",
    "            #---------   Average FOV   ----------\n",
    "            if not ref_matches:\n",
    "                print(f\"    No reference average found in {ref_dir}\")\n",
    "            else:\n",
    "                ref_file = ref_matches[0]  # take the first match\n",
    "                # destination in analysis folder\n",
    "                analysis_dir = info.recordingList.analysispathname.iloc[ind]\n",
    "                avg_path = os.path.join(\n",
    "                    analysis_dir,\n",
    "                    os.path.basename(ref_file))\n",
    "                avg_path = avg_path.replace(\".tif\", \".png\") \n",
    "\n",
    "                if not os.path.exists(avg_path):\n",
    "                    mfun.save_png_with_contrast(ref_file, avg_path, also_save_16bit=False)\n",
    "                    #shutil.copy2(ref_file, avg_path)  # copy with metadata\n",
    "                    print(f\"    Copied reference avg to {avg_path}\")\n",
    "                else:\n",
    "                    print(f\"    Reference avg already exists. Skipping...\")\n",
    "\n",
    "            # --------- SUITE2P CHECK / RUN ----------\n",
    "            suite2p_dir = os.path.join(tiff_path, \"suite2p\")\n",
    "            if  (not suite2p_reCalculate) and os.path.isdir(suite2p_dir):\n",
    "                info.recordingList.loc[ind, 'suite2Pcreated'] = 1\n",
    "                print(f\"    suite2p found. Skipping..\")\n",
    "            elif suite2p_reCalculate:\n",
    "                print(f\"    suite2p not found. Running extraction for: {tiff_path}\")\n",
    "                try:\n",
    "                    filenameENV = glob.glob(os.path.join(tiff_path, \"*.env\"))\n",
    "                    imagingDetails = mfun.parse_pv_env(filenameENV[0])\n",
    "                    mfun.suite2p_extraction(tiff_path,\n",
    "                                            ops_yaml_path=info.ops_yaml_path,\n",
    "                                            imagingDetails=imagingDetails)\n",
    "                # mfun.suite2p_extraction(tiff_path)\n",
    "                    info.recordingList.loc[ind, 'suite2Pcreated'] = 1\n",
    "                except Exception as e:\n",
    "                    print(f\"    suite2p_extraction failed for {tiff_path}: {e}\")\n",
    "                # set flag based on result on disk\n",
    "                info.recordingList.loc[ind, 'suite2Pcreated'] = 1\n",
    "            else:\n",
    "                info.recordingList.loc[ind, 'suite2Pcreated'] = 0\n",
    "                print(f\"    suite2p not found and not calculated.\")\n",
    "\n",
    "            # ###### STEP 4: create imaging files\n",
    "            print('Step 4: calculate & create imaging.pkl')\n",
    "\n",
    "            # Read suite2p\n",
    "            s2p_path = os.path.join(info.recordingList.path[ind], 'TwoP', f\"{info.recordingList.recordingDate[ind]}_t-001\", 'suite2p', 'plane0')\n",
    "            filenameDFF = os.path.join(info.recordingList.analysispathname[ind], 'imaging-data.pkl')\n",
    "            if (not dff_reCalculate) and os.path.exists(filenameDFF):\n",
    "                print(f\"    imaging-data.pkl already exists. Skipping...\")\n",
    "                info.recordingList.loc[ind, 'dffcreated'] = 1\n",
    "\n",
    "            elif dff_reCalculate & (os.path.exists(s2p_path)) & (info.recordingList.loc[ind, 'suite2Pcreated'] == 1):\n",
    "                print('    Loading suite2p data...')\n",
    "                ops = np.load(os.path.join(s2p_path, 'ops.npy'), allow_pickle=True)\n",
    "                ops = ops.item()\n",
    "                FrameNums = ops['frames_per_file']\n",
    "                filelist = ops['filelist']\n",
    "                isCell = np.load(os.path.join(s2p_path, 'iscell.npy'), allow_pickle=True)\n",
    "\n",
    "                # Load the suite2p\n",
    "                flu_raw_subtracted, spks, stat = utils.s2p_loader(s2p_path)\n",
    "                flu = utils.dfof2(flu_raw_subtracted)\n",
    "\n",
    "                # Cut each session & save it in the analysis-session folder\n",
    "                imaging_data = {\n",
    "                    \"n_frames\": FrameNums,\n",
    "                    \"flu\": flu,\n",
    "                    \"spks\": spks,\n",
    "                    \"stat\": stat,\n",
    "                }\n",
    "                \n",
    "                print(f\"{flu.shape[0]} cells; {flu.shape[1]} frames\")\n",
    "                with open(filenameDFF, 'wb') as f:\n",
    "                    pickle.dump(imaging_data, f)\n",
    "                print(f\"    Successfully saved imaging data.\")\n",
    "                create_FOV_withSelectedCells(isCell, ops, s2p_path)\n",
    "                print(f\"    Successfully saved FOV with selected cells.\")\n",
    "                calculateHeatmapsForRewardedStimulus()\n",
    "                print(f\"    Successfully saved heatmaps.\")\n",
    "                info.recordingList.loc[ind, 'dffcreated'] = 1\n",
    "            else:\n",
    "                print(f\"Suite2p path not found: {s2p_path}\")\n",
    "                info.recordingList.loc[ind, 'dffcreated'] = 0   \n",
    "            end_time = time.time()  \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"{ind} Successfully completed in {elapsed:.1f} seconds\\n\")   \n",
    "\n",
    "finally:\n",
    "    tee.close()   # important to restore stdout/stderr and close file\n",
    "\n",
    "# display the output\n",
    "print( \"Behaviour trial data extraction completed: \" + \n",
    "      str(info.recordingList['dffcreated'].sum()) +\"/\" + str(info.recordingList.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10> Save info into the analysis folder\n",
    "filenameINFO = info.analysisPath + '\\\\infoForAnalysis.pkl'\n",
    "with open(filenameINFO, 'wb') as f:\n",
    "    pickle.dump(info, f)\n",
    "print('All should be done!!')\n",
    "\n",
    "# Save table as CSV\n",
    "recordingList = info.recordingList\n",
    "recordingList.to_csv( info.analysisPath +'\\\\recordingList.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decMaking310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
