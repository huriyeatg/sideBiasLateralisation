{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takes <30s. However, it might take >1 mins or so if network is busy...\n",
      "Computer: Candela Windows\n",
      "Total Session: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lak Lab\\Documents\\Github\\sideBiasLateralisation\\main_funcs.py:145: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'C:/Users/Lak Lab/Documents/Github/sideBiasLateralisation\\analysis\\2025-04-07_CMN001_1\\' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  self.recordingList.loc[ind,'analysispathname'] = analysispathname +'\\\\'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matlab engine is set correctly.\n"
     ]
    }
   ],
   "source": [
    "#load the environment + define the path\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#make data file\n",
    "print( \"Takes <30s. However, it might take >1 mins or so if network is busy...\")\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import main_funcs as mfun\n",
    "import utils_funcs as utils # utils is from Vape - catcher file: \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# Get the list of recordings\n",
    "info = mfun.analysis()\n",
    "# display the detected session\n",
    "print( \"Total Session: \" +  str(info.recordingList .shape[0]))\n",
    "#info.recordingList.head()\n",
    "\n",
    "# set matlab API\n",
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "print('Matlab engine is set correctly.')\n",
    "\n",
    "info.recordingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animalID</th>\n",
       "      <th>recordingDate</th>\n",
       "      <th>recordingID</th>\n",
       "      <th>sessionName</th>\n",
       "      <th>learningData</th>\n",
       "      <th>twoP</th>\n",
       "      <th>path</th>\n",
       "      <th>sessionNameWithPath</th>\n",
       "      <th>analysispathname</th>\n",
       "      <th>filepathname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMN001</td>\n",
       "      <td>2025-04-09</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-09_1_CMN001</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Z:/CMN001\\2025-04-09</td>\n",
       "      <td>Z:/CMN001\\2025-04-09\\1\\2025-04-09_1_CMN001_Blo...</td>\n",
       "      <td>C:/Users/Lak Lab/Documents/Github/sideBiasLate...</td>\n",
       "      <td>Z:/CMN001\\2025-04-09\\1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  animalID recordingDate recordingID          sessionName  learningData  twoP  \\\n",
       "0   CMN001    2025-04-09           1  2025-04-09_1_CMN001         False  True   \n",
       "\n",
       "                   path                                sessionNameWithPath  \\\n",
       "0  Z:/CMN001\\2025-04-09  Z:/CMN001\\2025-04-09\\1\\2025-04-09_1_CMN001_Blo...   \n",
       "\n",
       "                                    analysispathname            filepathname  \n",
       "0  C:/Users/Lak Lab/Documents/Github/sideBiasLate...  Z:/CMN001\\2025-04-09\\1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for a particular expRef from the info list\n",
    "specific_expRef = '2025-04-09_1_CMN001'\n",
    "\n",
    "# Recreate info object with the specific expRef\n",
    "info.recordingList = info.recordingList[info.recordingList['sessionName'] == specific_expRef].reset_index(drop=True)\n",
    "info.recordingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for session 2025-05-09_1_MBL015, skipping visualization\n",
      "Existing file: Z:/MBL015\\2025-05-09\\2025-05-09_1_MBL015_behavior_plot.png\n"
     ]
    }
   ],
   "source": [
    "# create + save beh fig\n",
    "\n",
    "for ind, recordingDate in enumerate(info.recordingList.recordingDate):\n",
    "    session = str(info.recordingList.sessionName[0])\n",
    "    save_path = info.recordingList.path[0]  # Get the path where to save\n",
    "    save_file = os.path.join(save_path, f\"{session}_behavior_plot.png\")\n",
    "\n",
    "    # Check if file already exists\n",
    "    if os.path.exists(save_file):\n",
    "        print(f\"File already exists for session {session}, skipping visualization\")\n",
    "        print(f\"Existing file: {save_file}\")\n",
    "    else:\n",
    "        try:\n",
    "            # Create figure and get handle\n",
    "            eng.visualiseTrainingGrating2AFC(session, 'Grating2AFC_noTimeline', [], nargout=0)\n",
    "            \n",
    "            # Get the current figure handle\n",
    "            fig = eng.gcf()\n",
    "            \n",
    "            # Save the figure\n",
    "            eng.saveas(fig, save_file, nargout=0)\n",
    "            eng.close(fig, nargout=0)  # Close the figure\n",
    "            \n",
    "            print(f\"Visualization completed and saved for session {session}\")\n",
    "            print(f\"Saved to: {save_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error when running visualiseTrainingGrating2AFC: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to check the paq file details - change filenamePAQ\n",
    "# import paq2py as paq_reader\n",
    "# import utils_funcs as utils\n",
    "\n",
    "# filenamePAQ =  'Y:\\\\OFZ011\\\\2023-07-07\\\\TwoP\\\\2023-07-07_OFZ011_paq_001.paq'\n",
    "# savepathname = info.recordingList()\n",
    "# paq_data = paq_reader.paq_read( file_path=filenamePAQ, plot=True, save_path=savepathname) \n",
    "# frame_clock = utils.paq_data (paqData, 'reward', threshold_ttl=True, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behaviour trial data extraction completed: 1.0/1\n"
     ]
    }
   ],
   "source": [
    "# Check CSV files \n",
    "# checkOnly needs to be False to create new files with MATLAB GENERIC CODE: GetBehavData.mat\n",
    "checkOnly = False # Make false when there is more behaviour session\n",
    "alignSubtract = True\n",
    "#ind = 49\n",
    "for ind, recordingDate in enumerate(info.recordingList.recordingDate):\n",
    "#if ind ==49:\n",
    "    filenameCSV = info.recordingList.analysispathname[ind] + info.recordingList.sessionName[ind] + '_CorrectedeventTimes.csv'\n",
    "    e_filenameCSV = [f for f in glob.glob(filenameCSV)]\n",
    "    if len(e_filenameCSV)==1:\n",
    "        info.recordingList.loc[ind,'eventTimesExtracted']=1\n",
    "        info.recordingList.loc[ind,'eventTimesPath'] = filenameCSV\n",
    "    else:\n",
    "        if checkOnly:\n",
    "            info.recordingList.loc[ind,'eventTimesExtracted']=0\n",
    "            info.recordingList.loc[ind,'eventTimesPath'] = filenameCSV\n",
    "        else:\n",
    "            try:\n",
    "                filenameTimeline = [f for f in glob.glob(info.recordingList.filepathname[ind]+ '\\\\' + info.recordingList.sessionName[ind] + '_Timeline.mat')]\n",
    "                if  (len(filenameTimeline)>0):\n",
    "                    sessionProfile ='Grating2AFC'\n",
    "                else:\n",
    "                    sessionProfile ='Grating2AFC_noTimeline'\n",
    "                \n",
    "                # Get behaviour trial data from Block.mat file\n",
    "                print(' Extracting time events--> Profile: ' + sessionProfile  +'  Session: ' + info.recordingList.sessionName[ind])\n",
    "                data = eng.getBehavData(info.recordingList.sessionName[ind],sessionProfile)\n",
    "\n",
    "                # Apply correction based on the weights to match the eventTimes\n",
    "                if (alignSubtract) & (len(filenameTimeline)>0) & (info.recordingList.twoP[ind]==True): \n",
    "                    print(' Aligning time events: ' + info.recordingList.sessionName[ind])\n",
    "                    # Get weights to convert from probe to behavioural timebase\n",
    "                    twoPpath = info.recordingList.path[ind] + '\\\\TwoP'\n",
    "                    sessionName = info.recordingList.sessionName[ind]\n",
    "                    figsavepath = info.recordingList.analysispathname[ind]\n",
    "                    dataCorrected, variance = eng.applySubtractionCorrection (data, twoPpath ,sessionName, True, figsavepath, nargout=2)\n",
    "                    info.recordingList.loc[ind,'variance'] = variance\n",
    "\n",
    "                    # Apply correction to the signal\n",
    "                    data = dataCorrected\n",
    "                    \n",
    "                # Save the file\n",
    "                eng.writetable(data, filenameCSV, nargout=0)\n",
    "                info.recordingList.loc[ind,'eventTimesExtracted']=1\n",
    "            except:\n",
    "                print(str(ind) + ' - FAILED: Extracting time events: ' + info.recordingList.sessionName[ind])\n",
    "                info.recordingList.loc[ind,'eventTimesExtracted']=0\n",
    "\n",
    "# display the output\n",
    "print( \"Behaviour trial data extraction completed: \" + \n",
    "      str(info.recordingList.eventTimesExtracted.sum()) +\"/\" + str(info.recordingList.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default settings for Suite2p analysis\n",
    "ops = {\n",
    "        'batch_size': 200, # reduce if running out of RAM\n",
    "        'fast_disk': os.path.expanduser('~/suite2p_binaries'), # used to store temporary binary file, defaults to save_path0 (set as a string NOT a list)\n",
    "         #'save_path0': '/media/jamesrowland/DATA/plab/suite_2p', # stores results, defaults to first item in data_path\n",
    "        'delete_bin': False, # whether to delete binary file after processing\n",
    "        # main settings\n",
    "        'nplanes' : 1, # each tiff has these many planes in sequence\n",
    "        'nchannels' : 1, # each tiff has these many channels per plane\n",
    "        'functional_chan' : 1, # this channel is used to extract functional ROIs (1-based)\n",
    "        'diameter': 12, # this is the main parameter for cell detection, 2-dimensional if Y and X are different (e.g. [6 12])\n",
    "        'tau':  1.26, # this is the main parameter for deconvolution (1.25-1.5 for gcamp6s)\n",
    "        'fs': 30.,  # sampling rate (total across planes)\n",
    "        # output settings\n",
    "        'save_mat': True, # whether to save output as matlab files\n",
    "        'combined': True, # combine multiple planes into a single result /single canvas for GUI\n",
    "        # parallel settings\n",
    "        'num_workers': 0, # 0 to select num_cores, -1 to disable parallelism, N to enforce value\n",
    "        'num_workers_roi': 0, # 0 to select number of planes, -1 to disable parallelism, N to enforce value\n",
    "        # registration settings\n",
    "        'do_registration': True, # whether to register data\n",
    "        'nimg_init': 200, # subsampled frames for finding reference image\n",
    "        'maxregshift': 0.1, # max allowed registration shift, as a fraction of frame max(width and height)\n",
    "        'align_by_chan' : 1, # when multi-channel, you can align by non-functional channel (1-based)\n",
    "        'reg_tif': True, # whether to save registered tiffs\n",
    "        'subpixel' : 10, # precision of subpixel registration (1/subpixel steps)\n",
    "        # cell detection settings\n",
    "        'connected': True, # whether or not to keep ROIs fully connected (set to 0 for dendrites)\n",
    "        'navg_frames_svd': 5000, # max number of binned frames for the SVD\n",
    "        'nsvd_for_roi': 1000, # max number of SVD components to keep for ROI detection\n",
    "        'max_iterations': 20, # maximum number of iterations to do cell detection\n",
    "        'ratio_neuropil': 6., # ratio between neuropil basis size and cell radius\n",
    "        'ratio_neuropil_to_cell': 3, # minimum ratio between neuropil radius and cell radius\n",
    "        'tile_factor': 1., # use finer (>1) or coarser (<1) tiles for neuropil estimation during cell detection\n",
    "        'threshold_scaling': 1., # adjust the automatically determined threshold by this scalar multiplier\n",
    "        'max_overlap': 0.75, # cells with more overlap than this get removed during triage, before refinement\n",
    "        'inner_neuropil_radius': 2, # number of pixels to keep between ROI and neuropil donut\n",
    "        'outer_neuropil_radius': np.inf, # maximum neuropil radius\n",
    "        'min_neuropil_pixels': 350, # minimum number of pixels in the neuropil\n",
    "        # deconvolution settings\n",
    "        'baseline': 'maximin', # baselining mode\n",
    "        'win_baseline': 60., # window for maximin\n",
    "        'sig_baseline': 10., # smoothing constant for gaussian filter\n",
    "        'prctile_baseline': 8.,# optional (whether to use a percentile baseline)\n",
    "        'neucoeff': .7,  # neuropil coefficient\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMN0012025-04-09: Suite2p is created before\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Run Suite2p\n",
    "\n",
    "for ind, recordingDate in enumerate(info.recordingList.recordingDate):\n",
    "    # Check if twoP is True\n",
    "    if info.recordingList.twoP[ind]:\n",
    "        tiff_directory = os.path.join(info.recordingList.path[ind], 'TwoP', f\"{info.recordingList.recordingDate[ind]}_t-001\")\n",
    "        suite2p_folder = os.path.join(tiff_directory, 'suite2p')\n",
    "        e_suite2p_folder = [f for f in glob.glob(suite2p_folder)]\n",
    "        if len(e_suite2p_folder)==1:\n",
    "                info.recordingList.loc[ind,'suite2pPath'] = suite2p_folder\n",
    "                print(f\"{info.recordingList.animalID[ind]}{info.recordingList.recordingDate[ind]}: Suite2p is created before\")\n",
    "        else:\n",
    "            db = { \n",
    "                        'data_path':  os.path.join(info.rawPath, info.recordingList.animalID[ind]),\n",
    "                        'tiff_list': glob.glob(os.path.join(tiff_directory, \"*.tif\")),\n",
    "                        'save_folder': suite2p_folder\n",
    "                        }\n",
    "            from suite2p.run_s2p import run_s2p\n",
    "            import time        \n",
    "            t1 = time.time()\n",
    "            opsEnd = run_s2p(ops=ops,db=db)\n",
    "            t2 = time.time()\n",
    "            print(f\"{info.recordingList.animalID[ind]}: Suite2p is created in {t2 - t1} seconds.\")\n",
    "    else:\n",
    "        print(f\"Skipping Suite2p for {info.recordingList.animalID[ind]} - twoP is False\")\n",
    "    \n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting...\n",
      "loading 537 traces labelled as cells\n",
      "subtracting neuropil with a coefficient of 0.7\n",
      "C:/Users/Lak Lab/Documents/Github/sideBiasLateralisation\\analysis\\2025-04-09_CMN001_1\\imaging-data.pkl --> # of cells: 537 # of frames: 26211\n",
      "Successfully saved imaging data for session 2025-04-09_1_CMN001\n"
     ]
    }
   ],
   "source": [
    "#extract suite2p output\n",
    "\n",
    "for ind, recordingDate in enumerate(info.recordingList.recordingDate):\n",
    "    # Define the output file path\n",
    "    filenameINFO = os.path.join(info.recordingList.analysispathname[ind], 'imaging-data.pkl')\n",
    "    \n",
    "    # Check if file already exists\n",
    "    if os.path.exists(filenameINFO):\n",
    "        print(f\"File already exists for session {info.recordingList.sessionName[ind]}, skipping extraction\")\n",
    "        print(f\"Existing file: {filenameINFO}\")\n",
    "    else:\n",
    "        try:\n",
    "            # Read suite2p\n",
    "            s2p_path = os.path.join(info.recordingList.path[ind], 'TwoP', f\"{info.recordingList.recordingDate[ind]}_t-001\", 'suite2p', 'plane0')\n",
    "            if os.path.exists(s2p_path):\n",
    "                print('Extracting...')\n",
    "                ops = np.load(os.path.join(s2p_path, 'ops.npy'), allow_pickle=True)\n",
    "                ops = ops.item()\n",
    "                FrameNums = ops['frames_per_file']\n",
    "                filelist = ops['filelist']\n",
    "                isCell = np.load(os.path.join(s2p_path, 'iscell.npy'), allow_pickle=True)\n",
    "\n",
    "                # Load the suite2p\n",
    "                flu_raw_subtracted, spks, stat = utils.s2p_loader(s2p_path)\n",
    "                flu = utils.dfof2(flu_raw_subtracted)\n",
    "\n",
    "                # Cut each session & save it in the analysis-session folder\n",
    "                imaging_data = {\n",
    "                    \"n_frames\": FrameNums,\n",
    "                    \"flu\": flu,\n",
    "                    \"spks\": spks,\n",
    "                    \"stat\": stat,\n",
    "                }\n",
    "                \n",
    "                print(f\"{filenameINFO} --> # of cells: {flu.shape[0]} # of frames: {flu.shape[1]}\")\n",
    "                with open(filenameINFO, 'wb') as f:\n",
    "                    pickle.dump(imaging_data, f)\n",
    "                print(f\"Successfully saved imaging data for session {info.recordingList.sessionName[ind]}\")\n",
    "            else:\n",
    "                print(f\"Suite2p path not found: {s2p_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {info.recordingList.sessionName[ind]}: {str(e)}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imaging data extraction completed: 1.0/1\n"
     ]
    }
   ],
   "source": [
    "# Check Suite2p extraction\n",
    "\n",
    "for ind, recordingDate in enumerate(info.recordingList.recordingDate):\n",
    "    filenameDFF = info.recordingList.analysispathname[ind] + 'imaging-data.pkl'\n",
    "    e_filenameDFF = [f for f in glob.glob(filenameDFF)]\n",
    "    if len(e_filenameDFF)>0:\n",
    "        info.recordingList.loc[ind,'imagingDataExtracted']=1\n",
    "        info.recordingList.loc[ind,'imagingDataPath'] = filenameDFF\n",
    "    else:\n",
    "        info.recordingList.loc[ind,'imagingDataExtracted']=0\n",
    "        info.recordingList.loc[ind,'imagingDataPath'] = filenameDFF\n",
    "\n",
    "# display the output\n",
    "print( \"Imaging data extraction completed: \" + \n",
    "      str(info.recordingList.imagingDataExtracted.sum()) +\"/\" + str(info.recordingList.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys available in the file:\n",
      "dict_keys(['n_frames', 'flu', 'spks', 'stat'])\n",
      "\n",
      "Data information:\n",
      "Number of frames: [26211]\n",
      "Flu shape: (537, 26211)\n",
      "Spks shape: (537, 26211)\n",
      "Number of elements in stat: 537\n"
     ]
    }
   ],
   "source": [
    "# check imaging-data.pkl\n",
    "\n",
    "import pickle\n",
    "\n",
    "# File path\n",
    "file_path = info.recordingList.analysispathname[ind] + 'imaging-data.pkl'\n",
    "\n",
    "try:\n",
    "    # Open and load the pickle file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        imaging_data = pickle.load(f)\n",
    "    \n",
    "    # Show the keys of the dictionary\n",
    "    print(\"Keys available in the file:\")\n",
    "    print(imaging_data.keys())\n",
    "    \n",
    "    # Show basic data information\n",
    "    print(\"\\nData information:\")\n",
    "    print(f\"Number of frames: {imaging_data['n_frames']}\")\n",
    "    print(f\"Flu shape: {imaging_data['flu'].shape}\")\n",
    "    print(f\"Spks shape: {imaging_data['spks'].shape}\")\n",
    "    print(f\"Number of elements in stat: {len(imaging_data['stat'])}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found in {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error opening file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All should be done!!\n"
     ]
    }
   ],
   "source": [
    "# Save info into the analysis folder\n",
    "filenameINFO = info.analysisPath + '\\\\infoForAnalysis.pkl'\n",
    "with open(filenameINFO, 'wb') as f:\n",
    "    pickle.dump(info, f)\n",
    "print('All should be done!!')\n",
    "\n",
    "# Save table as CSV\n",
    "recordingList = info.recordingList\n",
    "recordingList.to_csv( info.analysisPath +'\\\\recordingList.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sideBias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
